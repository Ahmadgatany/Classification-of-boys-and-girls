{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6LPabt2tJ9A",
        "outputId": "740827db-e14d-4ac7-9a89-275d6759be32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['boys' 'girls' 'boys' 'girls' 'boys' 'girls' 'boys' 'girls' 'boys'\n",
            " 'girls']\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGSrIFa6tWT-",
        "outputId": "1e6f0082-6da7-4e84-9399-6b8892677d52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the base directory path\n",
        "base_directory = '/content/drive/My Drive/known_faces/'\n",
        "\n",
        "def load_and_resize_image(image_path, size=(150, 150)):\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path)\n",
        "    # Resize the image\n",
        "    img_resized = img.resize(size)\n",
        "    # Convert the image to a NumPy array\n",
        "    img_array = np.array(img_resized)\n",
        "    return img_array\n",
        "\n",
        "# List files in the directory\n",
        "files = os.listdir(base_directory)\n",
        "\n",
        "# Initialize lists to store images and labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Process each image\n",
        "for file_name in files:\n",
        "    image_path = os.path.join(base_directory, file_name)\n",
        "    try:\n",
        "        # Load and resize the image\n",
        "        image_array = load_and_resize_image(image_path)\n",
        "        # Append image data to X\n",
        "        X.append(image_array)\n",
        "        # Create target label based on the number in the filename\n",
        "        file_number = int(file_name.split('.')[0])\n",
        "        if file_number % 2 == 1:\n",
        "            y.append('boy')\n",
        "        else:\n",
        "            y.append('girl')\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Print the shape of X and y to verify\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Example of how the data looks\n",
        "print(f\"First label: {y[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2cEm3_vuOYS",
        "outputId": "58f01b63-4ad2-4b2d-fba3-5b5a955d553b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing .ipynb_checkpoints: [Errno 21] Is a directory: '/content/drive/My Drive/known_faces/.ipynb_checkpoints'\n",
            "X shape: (100, 150, 150, 3)\n",
            "y shape: (100,)\n",
            "First label: girl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Normalize image data\n",
        "X = X / 255.0\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "qlDJCdsFwBJy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "vaODV-R5yBMW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoded format\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_val_cat = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "qoTWARqhyHcH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(2, activation='softmax')  # Adjust the number of neurons based on the number of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETVuODZdyKuf",
        "outputId": "8405f08f-9d6f-48af-8187-8ecf457cd4cc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    epochs=15,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val_cat)\n",
        "print(f\"Validation accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7-ulFx5x6Zj",
        "outputId": "1a15bd69-d58e-44ad-e1d4-74586e331ce8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5403 - loss: 0.8571 - val_accuracy: 0.5667 - val_loss: 1.4197\n",
            "Epoch 2/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.4798 - loss: 1.3166 - val_accuracy: 0.4333 - val_loss: 0.7217\n",
            "Epoch 3/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 989ms/step - accuracy: 0.5371 - loss: 0.6888 - val_accuracy: 0.5667 - val_loss: 0.6928\n",
            "Epoch 4/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 997ms/step - accuracy: 0.6879 - loss: 0.6643 - val_accuracy: 0.5667 - val_loss: 0.6772\n",
            "Epoch 5/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.7119 - loss: 0.6176 - val_accuracy: 0.5667 - val_loss: 0.6523\n",
            "Epoch 6/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5670 - loss: 0.6001 - val_accuracy: 0.7667 - val_loss: 0.6111\n",
            "Epoch 7/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 961ms/step - accuracy: 0.8732 - loss: 0.5034 - val_accuracy: 0.6000 - val_loss: 0.5913\n",
            "Epoch 8/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.7041 - loss: 0.5026 - val_accuracy: 0.7333 - val_loss: 0.5606\n",
            "Epoch 9/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 994ms/step - accuracy: 0.8810 - loss: 0.3757 - val_accuracy: 0.7333 - val_loss: 0.5246\n",
            "Epoch 10/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9142 - loss: 0.3087 - val_accuracy: 0.7000 - val_loss: 0.6666\n",
            "Epoch 11/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.9623 - loss: 0.2132 - val_accuracy: 0.6667 - val_loss: 0.5409\n",
            "Epoch 12/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9512 - loss: 0.1610 - val_accuracy: 0.4333 - val_loss: 1.9255\n",
            "Epoch 13/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 983ms/step - accuracy: 0.8114 - loss: 0.3542 - val_accuracy: 0.7667 - val_loss: 0.6134\n",
            "Epoch 14/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.9259 - loss: 0.1966 - val_accuracy: 0.7000 - val_loss: 0.5037\n",
            "Epoch 15/15\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.1329 - val_accuracy: 0.6667 - val_loss: 0.9252\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 0.6667 - loss: 0.9252\n",
            "Validation accuracy: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('/content/drive/My Drive/cnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8Yo1mGHzW1h",
        "outputId": "74156f7c-4ff6-4229-f827-0478739a883c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create and fit the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)  # Fit on your training labels\n",
        "\n",
        "# Save the label encoder classes\n",
        "np.save('/content/drive/My Drive/label_encoder_classes.npy', label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "oxNqhHe00RqX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the label encoder classes\n",
        "label_classes = np.load('/content/drive/My Drive/label_encoder_classes.npy')\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = label_classes\n"
      ],
      "metadata": {
        "id": "tspboRjj0vww"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/cnn_model.h5')\n",
        "\n",
        "# Define the function to load, resize, and preprocess the new image\n",
        "def preprocess_image(image_path, size=(150, 150)):\n",
        "    img = Image.open(image_path)\n",
        "    img_resized = img.resize(size)\n",
        "    img_array = np.array(img_resized)\n",
        "    img_array = img_array / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Load the label encoder classes\n",
        "label_classes = np.load('/content/drive/My Drive/label_encoder_classes.npy')\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = label_classes\n",
        "\n",
        "# Path to the new image\n",
        "new_image_path = 'Ahmed.jpg'\n",
        "\n",
        "# Preprocess the new image\n",
        "new_image_array = preprocess_image(new_image_path)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(new_image_array)\n",
        "\n",
        "# Decode the prediction\n",
        "predicted_class = np.argmax(prediction, axis=1)\n",
        "predicted_label = label_encoder.inverse_transform(predicted_class)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Predicted label: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFmkMBXA1LvD",
        "outputId": "f89ccec8-e7d6-4048-b386-eb10f1d55888"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Predicted label: boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KpLDb6T41hks"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}